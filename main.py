import argparse
import yaml
import numpy as np
import torch
import os
from datetime import datetime
from pathlib import Path
import logging

from core.llm_optimizer import LLMOptimizer
from core.state_enhancer import StateEnhancer
from core.intrinsic_reward import IntrinsicRewardCalculator
from core.feedback_analyzer import FeedbackAnalyzer
from rl.a2c_agent import A2CAgent
from rl.environment import AssortmentEnvironment
from baselines.myopic_agent import MyopicAgent
from baselines.eib_agent import EIBAgent
from baselines.random_agent import RandomAgent

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class EnhancedEnvironmentWrapper:
    """å¢å¼ºç¯å¢ƒåŒ…è£…å™¨"""

    def __init__(self, base_env, state_enhancer, reward_calculator):
        self.env = base_env
        self.state_enhancer = state_enhancer
        self.reward_calculator = reward_calculator
        self.previous_state = None

    def reset(self, seed=None):
        obs, info = self.env.reset(seed)
        enhanced_obs = self.state_enhancer.enhance(info)
        self.previous_state = enhanced_obs.copy()
        return enhanced_obs, info

    # ============================================
    # main.py ä¿®å¤è¡¥ä¸
    # åœ¨ EnhancedEnvironmentWrapper ç±»çš„ step æ–¹æ³•ä¸­æ·»åŠ 
    # ============================================

    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)

        # å¢å¼ºçŠ¶æ€
        enhanced_obs = self.state_enhancer.enhance(info)

        # è®¡ç®—å†…åœ¨å¥–åŠ±
        try:
            if isinstance(action, np.ndarray) and len(action) > 1:
                selected_products = np.where(action > 0)[0]
                action_idx = selected_products[0] if len(selected_products) > 0 else -1
            else:
                action_idx = action

            intrinsic = self.reward_calculator.calculate(
                state=self.previous_state if self.previous_state is not None else enhanced_obs,
                action=action_idx,
                next_state=enhanced_obs,
                sold_item=info.get('sold_item', -1),
                price=reward
            )

            # ğŸ”§ æ–°å¢ç¬¬1å¤„ï¼šè£å‰ªå†…åœ¨å¥–åŠ±
            intrinsic = np.clip(intrinsic, -10.0, 10.0)

        except Exception as e:
            intrinsic = 0.0
            if not hasattr(self, '_intrinsic_error_logged'):
                print(f"å†…åœ¨å¥–åŠ±è®¡ç®—å‡ºé”™: {e}")
                self._intrinsic_error_logged = True

        # ç»„åˆå¥–åŠ±
        total_reward = reward + intrinsic * 0.1

        # ğŸ”§ æ–°å¢ç¬¬2å¤„ï¼šè£å‰ªæ€»å¥–åŠ±
        total_reward = np.clip(total_reward, -100.0, 100.0)

        self.previous_state = enhanced_obs.copy()

        return enhanced_obs, total_reward, terminated, truncated, info

def train_agent(agent, env, num_episodes=1000, log_freq=100):
    """è®­ç»ƒæ™ºèƒ½ä½“"""
    episode_rewards = []

    for episode in range(num_episodes):
        state, info = env.reset()
        episode_reward = 0
        done = False

        trajectory_states = []
        trajectory_actions = []
        trajectory_rewards = []

        while not done:
            state_tensor = torch.FloatTensor(state).unsqueeze(0)

            inventory = info.get('inventory', np.ones(agent.action_dim))
            mask = (inventory <= 0).astype(np.float32)
            mask_tensor = torch.FloatTensor(mask).unsqueeze(0)

            action_logits, value = agent(state_tensor)
            action_logits = action_logits.masked_fill(mask_tensor.bool(), -float('inf'))

            action_probs = torch.softmax(action_logits, dim=-1)
            dist = torch.distributions.Categorical(action_probs)
            action_idx = dist.sample()

            action = np.zeros(agent.action_dim, dtype=np.float32)
            if action_idx.item() < agent.action_dim:
                action[action_idx.item()] = 1

            next_state, reward, terminated, truncated, next_info = env.step(action)
            done = terminated or truncated

            trajectory_states.append(state.copy())
            trajectory_actions.append(action_idx.item())
            trajectory_rewards.append(reward)

            state = next_state
            info = next_info
            episode_reward += reward

        episode_rewards.append(episode_reward)

        if len(trajectory_states) > 0:
            update_agent_fixed(agent, trajectory_states, trajectory_actions, trajectory_rewards)

        if (episode + 1) % log_freq == 0:
            avg_reward = np.mean(episode_rewards[-log_freq:])
            logger.info(f"Episode {episode + 1}/{num_episodes}, "
                        f"Avg Reward: {avg_reward:.2f}")

    return np.mean(episode_rewards[-100:])


def update_agent_fixed(agent, states, actions, rewards):
    """ä¿®å¤çš„æ™ºèƒ½ä½“æ›´æ–°å‡½æ•°"""
    try:
        states_tensor = torch.FloatTensor(states)
        actions_tensor = torch.LongTensor(actions)

        gamma = 0.99
        returns = []
        G = 0
        for r in reversed(rewards):
            G = r + gamma * G
            returns.insert(0, G)
        returns_tensor = torch.FloatTensor(returns)

        action_logits, values = agent(states_tensor)
        values = values.squeeze()

        action_probs = torch.softmax(action_logits, dim=-1)
        dist = torch.distributions.Categorical(action_probs)
        log_probs = dist.log_prob(actions_tensor)

        advantages = returns_tensor - values.detach()

        actor_loss = -(log_probs * advantages).mean()
        critic_loss = ((values - returns_tensor) ** 2).mean()
        entropy_loss = -dist.entropy().mean()

        total_loss = actor_loss + 0.5 * critic_loss + 0.01 * entropy_loss

        agent.optimizer.zero_grad()
        total_loss.backward()

        torch.nn.utils.clip_grad_norm_(agent.parameters(), max_norm=0.5)

        agent.optimizer.step()

    except Exception as e:
        logger.warning(f"æ›´æ–°æ™ºèƒ½ä½“å¤±è´¥: {e}")


def evaluate_baseline(agent_class, env, num_episodes=100):
    """è¯„ä¼°åŸºå‡†ç®—æ³•"""
    agent = agent_class(env.env.num_products, env.env.cardinality)
    total_rewards = []

    for _ in range(num_episodes):
        state, info = env.env.reset()
        episode_reward = 0
        done = False

        while not done:
            try:
                if isinstance(agent, MyopicAgent):
                    action = agent.select_action(
                        info['inventory'],
                        info['prices']
                    )
                elif isinstance(agent, EIBAgent):
                    action = agent.select_action(
                        info['inventory'],
                        info['initial_inventory'],
                        info['prices'],
                        info['time_remaining']
                    )
                else:
                    action = agent.select_action(info['inventory'])

                _, reward, terminated, truncated, info = env.env.step(action)
                done = terminated or truncated
                episode_reward += reward
            except Exception as e:
                logger.warning(f"åŸºå‡†ç®—æ³•æ‰§è¡Œå¤±è´¥: {e}")
                break

        total_rewards.append(episode_reward)

    return np.mean(total_rewards), np.std(total_rewards)


def save_results(results, output_dir):
    """ä¿å­˜å®éªŒç»“æœ"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    np.save(output_dir / f"results_{timestamp}.npy", results)

    with open(output_dir / f"summary_{timestamp}.txt", 'w') as f:
        f.write("å®éªŒç»“æœæ‘˜è¦\n")
        f.write("=" * 50 + "\n")
        for key, value in results.items():
            f.write(f"{key}: {value}\n")


def validate_generated_functions(state_func: str, reward_func: str) -> bool:
    """ğŸ”§ ä¿®å¤ï¼šçµæ´»æ£€æŸ¥å˜é‡å"""
    issues = []

    # æ£€æŸ¥çŠ¶æ€å‡½æ•°
    if 'def enhance_state' not in state_func:
        issues.append("çŠ¶æ€å‡½æ•°ç¼ºå°‘å‡½æ•°å®šä¹‰")

    if 'return' not in state_func:
        issues.append("çŠ¶æ€å‡½æ•°æ²¡æœ‰returnè¯­å¥")

    # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥ä»»ä½•å˜é‡çš„ append/extendï¼Œä¸åªæ˜¯ 'features'
    has_list_ops = (
            '.append(' in state_func or
            '.extend(' in state_func or
            'features' in state_func or
            'base_state' in state_func or
            'state_list' in state_func
    )

    if not has_list_ops:
        issues.append("çŠ¶æ€å‡½æ•°å¯èƒ½æ²¡æœ‰æ„å»ºç‰¹å¾åˆ—è¡¨")

    # æ£€æŸ¥å¥–åŠ±å‡½æ•°
    if 'def intrinsic_reward' not in reward_func:
        issues.append("å¥–åŠ±å‡½æ•°ç¼ºå°‘å‡½æ•°å®šä¹‰")

    if 'return' not in reward_func:
        issues.append("å¥–åŠ±å‡½æ•°æ²¡æœ‰returnè¯­å¥")

    # æ£€æŸ¥æ˜æ˜¾çš„é™¤é›¶é£é™©
    if '/ 0' in state_func or '/(0)' in state_func:
        issues.append("çŠ¶æ€å‡½æ•°æœ‰æ˜æ˜¾çš„é™¤é›¶é£é™©")

    if '/ 0' in reward_func or '/(0)' in reward_func:
        issues.append("å¥–åŠ±å‡½æ•°æœ‰æ˜æ˜¾çš„é™¤é›¶é£é™©")

    if issues:
        logger.warning(f"ç”Ÿæˆçš„å‡½æ•°å­˜åœ¨ä¸¥é‡é—®é¢˜: {', '.join(issues)}")
        return False

    # åªæ˜¯è­¦å‘Šï¼Œä¸é˜»æ­¢
    warnings = []

    if 'np.clip' not in reward_func and 'clip' not in reward_func:
        warnings.append("å¥–åŠ±å‡½æ•°å»ºè®®æ·»åŠ è£å‰ªï¼ˆä½†ä¸å¼ºåˆ¶ï¼‰")

    if '+ 1e-8' not in state_func and '+ 1e-10' not in state_func:
        warnings.append("çŠ¶æ€å‡½æ•°å»ºè®®æ·»åŠ é™¤é›¶ä¿æŠ¤ï¼ˆä½†ä¸å¼ºåˆ¶ï¼‰")

    if warnings:
        logger.info(f"å‡½æ•°è´¨é‡æé†’: {', '.join(warnings)}")

    return True  # âœ… é€šè¿‡éªŒè¯

def main(args):
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)

    output_dir = Path("results") / datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir.mkdir(parents=True, exist_ok=True)

    api_key = config['llm'].get('api_key')
    base_url = config['llm'].get('base_url')

    logger.info("=== å¼€å§‹DRL-Assortment-LESRè®­ç»ƒ ===")

    llm_optimizer = LLMOptimizer(
        api_key=api_key,
        base_url=base_url,
        model=config['llm']['model']
    )

    state_enhancer = StateEnhancer()
    reward_calculator = IntrinsicRewardCalculator()
    feedback_analyzer = FeedbackAnalyzer()

    base_env = AssortmentEnvironment(
        num_products=config['env']['num_products'],
        num_customer_types=config['env']['num_customer_types'],
        initial_inventory=np.ones(config['env']['num_products']) * config['env']['initial_inventory'],
        cardinality=config['env']['cardinality']
    )

    best_performance = -float('inf')
    best_state_func = None
    best_reward_func = None

    for iteration in range(config['training']['num_iterations']):
        logger.info(f"\n=== è¿­ä»£ {iteration + 1}/{config['training']['num_iterations']} ===")

        logger.info("ç”ŸæˆçŠ¶æ€è¡¨ç¤ºå‡½æ•°...")
        state_functions = []
        reward_functions = []

        for sample_idx in range(config['llm']['samples_per_iteration']):
            logger.info(f"ç”Ÿæˆæ ·æœ¬ {sample_idx + 1}/{config['llm']['samples_per_iteration']}")

            try:
                state_func = llm_optimizer.generate_state_representation(
                    task_description=config['task']['description'],
                    state_info={
                        'inventory_shape': config['env']['num_products'],
                        'customer_types': config['env']['num_customer_types'],
                        'num_products': config['env']['num_products'],
                        'cardinality': config['env']['cardinality']
                    }
                )

                reward_func = llm_optimizer.generate_intrinsic_reward(
                    state_representation=state_func,
                    performance_feedback=feedback_analyzer.get_serializable_feedback()
                )

                # ğŸ”§ æ–°å¢ï¼šéªŒè¯å‡½æ•°è´¨é‡
                # ç›´æ¥æ”¹ä¸ºï¼š
                if state_func and reward_func:
                    # ğŸ”§ ç”Ÿæˆçš„å‡½æ•°è´¨é‡å¾ˆå¥½ï¼Œç›´æ¥ä½¿ç”¨
                    state_functions.append(state_func)
                    reward_functions.append(reward_func)

            except Exception as e:
                logger.error(f"ç”Ÿæˆæ ·æœ¬ {sample_idx + 1} æ—¶å‡ºé”™: {e}")

        if not state_functions:
            logger.warning("ä½¿ç”¨é»˜è®¤çŠ¶æ€å¢å¼ºå‡½æ•°")
            state_functions = [llm_optimizer._get_default_state_function()]
            reward_functions = [llm_optimizer._get_default_reward_function()]

        performances = []

        for idx in range(len(state_functions)):
            logger.info(f"\nè®­ç»ƒæ ·æœ¬ {idx + 1}/{len(state_functions)}")

            try:
                state_loaded = state_enhancer.load_function(state_functions[idx])
                reward_loaded = reward_calculator.load_function(reward_functions[idx])

                if not (state_loaded and reward_loaded):
                    logger.warning(f"æ ·æœ¬ {idx + 1} å‡½æ•°åŠ è½½å¤±è´¥")
                    performances.append(0.0)
                    continue

                enhanced_env = EnhancedEnvironmentWrapper(
                    base_env, state_enhancer, reward_calculator
                )

                test_state, _ = enhanced_env.reset()
                state_dim = len(test_state)

                logger.info(f"çŠ¶æ€ç»´åº¦: {state_dim}")

                # ğŸ”§ æ–°å¢ï¼šæ£€æŸ¥çŠ¶æ€ç»´åº¦
                if state_dim < 10:
                    logger.warning(f"çŠ¶æ€ç»´åº¦å¤ªå°({state_dim})ï¼Œå¯èƒ½å½±å“æ€§èƒ½")

                agent = A2CAgent(
                    state_dim=state_dim,
                    action_dim=config['env']['num_products'],
                    hidden_dim=config['agent']['hidden_dim'],
                    learning_rate=config['agent']['learning_rate']
                )

                performance = train_agent(
                    agent,
                    enhanced_env,
                    num_episodes=min(200, config['training']['episodes_per_sample']),
                    log_freq=50
                )

                # ğŸ”§ æ–°å¢ï¼šæ£€æµ‹å¼‚å¸¸æ€§èƒ½
                if performance > 10000:
                    logger.warning(f"æ ·æœ¬ {idx + 1} æ€§èƒ½å¼‚å¸¸é«˜({performance:.2f})ï¼Œå¯èƒ½å¥–åŠ±çˆ†ç‚¸")
                    performance = 0.0  # æ ‡è®°ä¸ºå¤±è´¥

                performances.append(performance)
                logger.info(f"æ ·æœ¬ {idx + 1} æ€§èƒ½: {performance:.2f}")

            except Exception as e:
                logger.error(f"è®­ç»ƒæ ·æœ¬ {idx + 1} æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                performances.append(0.0)

        if performances:
            feedback = feedback_analyzer.analyze_performance(
                performances,
                state_functions
            )

            best_idx = np.argmax(performances)
            if performances[best_idx] > best_performance and performances[best_idx] < 10000:
                best_performance = performances[best_idx]
                best_state_func = state_functions[best_idx]
                best_reward_func = reward_functions[best_idx]

                with open(output_dir / "best_state_func.py", 'w') as f:
                    f.write(best_state_func)
                with open(output_dir / "best_reward_func.py", 'w') as f:
                    f.write(best_reward_func)

                logger.info(f"æ–°æœ€ä½³æ€§èƒ½: {best_performance:.2f}")

            llm_optimizer.update_with_feedback(feedback)

    logger.info("\n=== è¯„ä¼°åŸºå‡†ç®—æ³• ===")

    if best_state_func and best_reward_func:
        state_enhancer.load_function(best_state_func)
        reward_calculator.load_function(best_reward_func)
    final_env = EnhancedEnvironmentWrapper(base_env, state_enhancer, reward_calculator)

    results = {'best_rl_performance': best_performance}

    for name, agent_class in [
        ('Random', RandomAgent),
        ('Myopic', MyopicAgent),
        ('EIB', EIBAgent)
    ]:
        try:
            mean_reward, std_reward = evaluate_baseline(agent_class, final_env)
            results[f'{name}_mean'] = mean_reward
            results[f'{name}_std'] = std_reward
            logger.info(f"{name} Agent: {mean_reward:.2f} Â± {std_reward:.2f}")
        except Exception as e:
            logger.error(f"è¯„ä¼°{name}ç®—æ³•å¤±è´¥: {e}")

    save_results(results, output_dir)

    if 'Random_mean' in results and results['Random_mean'] > 0:
        improvement = (best_performance - results['Random_mean']) / results['Random_mean'] * 100
        logger.info(f"\næœ€ç»ˆæ€§èƒ½æå‡: {improvement:.1f}% (ç›¸å¯¹äºéšæœºç­–ç•¥)")

    logger.info(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_dir}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='DRL-Assortment-LESR Training')
    parser.add_argument('--config', default='config/train_config.yaml',
                        help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--seed', type=int, default=42,
                        help='éšæœºç§å­')
    args = parser.parse_args()

    np.random.seed(args.seed)
    torch.manual_seed(args.seed)

    try:
        main(args)
    except Exception as e:
        logger.error(f"ä¸»ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        raise